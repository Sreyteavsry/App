{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreyteavsry/App/blob/master/Final_Version__5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdXy_KVNX4m5"
      },
      "source": [
        "**Importing the libraries**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_vQh4ff0xcq"
      },
      "outputs": [],
      "source": [
        "# !pip install colabcode\n",
        "# !pip install fastapi\n",
        "# !pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqCZ_hxf0_kP"
      },
      "outputs": [],
      "source": [
        "# from colabcode import ColabCode\n",
        "# from fastapi import FastAPI\n",
        "# app = FastAPI()\n",
        "# @app.get('/')\n",
        "# async def read_root():\n",
        "#   return {\"message\": \"Compound Identification\"}\n",
        "# server= ColabCode(port=10000, code=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArXQ-EEM1R8k"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from fastapi import FastAPI\n",
        "# from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# app = FastAPI()\n",
        "\n",
        "# app.add_middleware(\n",
        "#     CORSMiddleware,\n",
        "#     allow_origins=['*'],\n",
        "#     allow_credentials=True,\n",
        "#     allow_methods=['*'],\n",
        "#     allow_headers=['*'],\n",
        "# )\n",
        "\n",
        "# @app.get('/')\n",
        "# async def root():\n",
        "#     return {'hello': 'world'}#  server.run_app(app=app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVLM5tBqPtB5"
      },
      "outputs": [],
      "source": [
        "# import nest_asyncio\n",
        "# from pyngrok import ngrok\n",
        "# import uvicorn\n",
        "\n",
        "# ngrok_tunnel = ngrok.connect(8000)\n",
        "# print('Public URL:', ngrok_tunnel.public_url)\n",
        "# nest_asyncio.apply()\n",
        "# uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBm7RA6x2BOv"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtPqs7I5XrGB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve4JX3noZ_Bn"
      },
      "source": [
        "**Checking for available GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQRW5QgzYr_V",
        "outputId": "54554b46-56db-42c2-af76-3be9868820c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU\n"
          ]
        }
      ],
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "  print(\"Training on GPU\")\n",
        "else:\n",
        "  print(\"No GPU available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf44TBLiCwsb"
      },
      "source": [
        "#Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O0dEogkC9D1"
      },
      "source": [
        "##Loading files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dcJaXeM31yo"
      },
      "outputs": [],
      "source": [
        "def compound_tag(str): #remove the combinded tags (compound words)\n",
        "    ls = ['n-[', 'o-[','v-[','v[','o[', 'n[' ,'a-[', 'a[','1[','.[','+[', '_n-','_n', '_v', '_o-','_o','_a-', '_a','_1-','_.-','_1', '_.']\n",
        "    #ls = ['o[', 'n[', 'n-[', 'o-[','v-[','v[','a-[', 'a[','1[','.[','+[',']n-',']n',']o-',']v-',']v',']o',']a-',']a',']1',']o-','].', ']+','_n', '_n-', '_v', '_o','_a','_1', '_.']\n",
        "    cm = [']n-',']n',']o-',']v-',']v',']o',']a-',']a',']1',']o-','].', ']+']\n",
        "    for i in ls:\n",
        "      str = str.replace(i,'')\n",
        "    for ii in cm:\n",
        "      str = str.replace(']', '_c')\n",
        "    return str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqgdM9UfqAya"
      },
      "outputs": [],
      "source": [
        "def cls(str): #remove the combinded tags (compound words)\n",
        "    ls = ['o[', 'n[', 'n-[', 'o-[','v-[','v[','a-[', 'a[','1[','.[','+[',']n-',']n',']o-',']v-',']v',']o',']a-',']a',']1',']o-','].', ']+']\n",
        "    for i in ls:\n",
        "      str = str.replace(i,'')\n",
        "    return str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55E1dOLFDwtv",
        "outputId": "e3a828fa-e605-4af7-c2d4-d0bafcebe4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "f_tok = open('/content/drive/MyDrive/Dataset/Khmer NLP/data_km.km-tok.txt', 'r', encoding ='utf-8')\n",
        "\n",
        "f_tag = open('/content/drive/MyDrive/Dataset/Khmer NLP/data_km.km-tag.txt', 'r', encoding ='utf-8')\n",
        "tok = [x.split() for x in f_tok] #split each sentence by space into word list\n",
        "postag = [x.split() for x in f_tag] #split each sentence by space into tag list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8GabosCGkbA"
      },
      "source": [
        "## Combining token&tag and removing number of sentence (ex: SNT.80188.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tomjCnoH3jV"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "tok_clean = []\n",
        "tag_clean = []\n",
        "tag_clean_all =[]\n",
        "for ii in range(len(tok)):\n",
        "# initializing lists  \n",
        "  tok_list = tok[ii]\n",
        "  tag_list = postag[ii]\n",
        "  tag_list_clean = [cls(i) for i in postag[ii]]\n",
        "  res = [i + '_'+ j for i, j in zip(tok_list, tag_list)]\n",
        "  res.pop(0)\n",
        "  data.append(res)\n",
        "  tok_clean.append(tok_list[1:])\n",
        "  tag_clean.append(tag_list[1:])\n",
        "  tag_clean_all.append(tag_list_clean[1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyntp4k4FdA7",
        "outputId": "f74a9596-469a-4da9-f204-8b8318106766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['n', 'v-', 'v', 'o', 'n', 'n', 'o', 'n', 'n', 'o', 'n', 'v', 'n', 'n', 'n', 'n', 'o', 'n', 'n', 'v', 'n', '1', 'n', 'v', 'v-', 'n', 'n', 'n', 'n', 'n', 'n', '.']\n"
          ]
        }
      ],
      "source": [
        "print(tag_clean_all[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiYI_2kGycPJ"
      },
      "source": [
        "## List of characters and tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSafituJy52R"
      },
      "outputs": [],
      "source": [
        "KHCONST = list(u'កខគឃងចឆជឈញដឋឌឍណតថទធនបផពភមយរលវឝឞសហឡអឣឤឥឦឧឩឪឫឬឭឮឯឰឱឲឳ')\n",
        "KHVOWEL = list(u'឴឵ាិីឹឺុូួើឿៀេែៃោៅ\\u17c6\\u17c7\\u17c8')\n",
        "# subscript, diacritics\n",
        "KHSUB = list(u'្')\n",
        "KHDIAC = list(u\"\\u17c9\\u17ca\\u17cb\\u17cc\\u17cd\\u17ce\\u17cf\\u17d0\\u200b\") #MUUSIKATOAN, TRIISAP, BANTOC,ROBAT,\n",
        "KHSYM = list('៕។៛ៗ៚៙៘,.?-!') # add space\n",
        "KHNUMBER = list(u'០១២៣៤៥៦៧៨៩0123456789') # remove 0123456789\n",
        "ENGCHAR = list(u'abcdefghijklmnopqrstuvwxyz')\n",
        "ENGUPPERCHAR = list(u'ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "\n",
        "CHARS = ['PADDING'] + ['UNK']+ KHCONST + KHVOWEL + KHSUB + KHDIAC + KHSYM + KHNUMBER + ENGCHAR + ENGUPPERCHAR\n",
        "NOTAG = ['ns']\n",
        "NOTAGCM = ['cns']\n",
        "BASICTAGS = list(('n','v', 'a', 'o'))\n",
        "AUXTAGS = list(('1', '.', '+'))\n",
        "MODIFIEDTAGS =list(('n-', 'v-', 'a-', 'o-'))\n",
        "COMPOUNDTAGS = list(('cn','cv', 'ca', 'co', 'cn-', 'cv-', 'ca-', 'co-', 'c1', 'c.', 'c+'))\n",
        "POSTAGS = ['PADDING']+ NOTAG + BASICTAGS + AUXTAGS + MODIFIEDTAGS + NOTAGCM+ COMPOUNDTAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeV5XfoZy7X-"
      },
      "source": [
        "## Mapping char and tag to an index and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FR4X2RzzDG8",
        "outputId": "1bee1a60-b8ac-4091-8d3a-a0fb508d8954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PADDING': 0, 'ns': 1, 'n': 2, 'v': 3, 'a': 4, 'o': 5, '1': 6, '.': 7, '+': 8, 'n-': 9, 'v-': 10, 'a-': 11, 'o-': 12, 'cns': 13, 'cn': 14, 'cv': 15, 'ca': 16, 'co': 17, 'cn-': 18, 'cv-': 19, 'ca-': 20, 'co-': 21, 'c1': 22, 'c.': 23, 'c+': 24}\n"
          ]
        }
      ],
      "source": [
        "chars2idx = {o:i for i,o in enumerate(CHARS)}\n",
        "tags2idx = {o:i for i,o in enumerate(POSTAGS)}\n",
        "idx2tags = {i:o for i,o in enumerate(POSTAGS)}\n",
        "idx2chars = {i:o for i,o in enumerate(CHARS)}\n",
        "print(tags2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36KFw3QMGRn3"
      },
      "source": [
        "##Generating characters with labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw6oh-9xGgWk"
      },
      "outputs": [],
      "source": [
        "def gen_char_with_label(sentence, tags):\n",
        "    words = sentence\n",
        "    final_kccs = []\n",
        "    for word, tagss in zip(words, tags):\n",
        "        kccs = list(word)\n",
        "        labels = [cls(tagss) if (i==0 or k==\" \") else 'ns' for i, k in enumerate(word)]\n",
        "        final_kccs.extend(list(zip(kccs,labels)))\n",
        "    return final_kccs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzFphmVJAvav"
      },
      "outputs": [],
      "source": [
        "def gen_char_with_label_compound(sentence, tags):\n",
        "    words = sentence\n",
        "    final_kccs = []\n",
        "    for word, tagss in zip(words, tags):\n",
        "        kccs = list(word)\n",
        "        labels = [cls(tagss) if (i==0 or k==\" \") else 'cns' for i, k in enumerate(word)]\n",
        "        final_kccs.extend(list(zip(kccs,labels)))\n",
        "    return final_kccs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBaJIv0HRqE",
        "outputId": "ddfb0214-f629-4018-e63a-979d09ceaed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('អ', 'n'), ('៊', 'ns'), ('ី', 'ns'), ('ត', 'ns'), ('ា', 'ns'), ('ល', 'ns'), ('ី', 'ns'), ('ប', 'v-'), ('ា', 'ns'), ('ន', 'ns'), ('ឈ', 'v'), ('្', 'ns'), ('ន', 'ns'), ('ះ', 'ns'), ('ល', 'o'), ('ើ', 'ns'), ('ព', 'n'), ('័', 'ns'), ('រ', 'ns'), ('ទ', 'ns'), ('ុ', 'ns'), ('យ', 'ns'), ('ហ', 'ns'), ('្', 'ns'), ('គ', 'ns'), ('ា', 'ns'), ('ល', 'ns'), ('់', 'ns'), ('3', 'n'), ('1', 'ns'), ('-', 'ns'), ('5', 'ns'), ('ក', 'o'), ('្', 'ns'), ('ន', 'ns'), ('ុ', 'ns'), ('ង', 'ns'), ('ប', 'n'), ('៉', 'ns'), ('ូ', 'ns'), ('ល', 'ns'), ('C', 'n'), ('ន', 'o'), ('ៃ', 'ns'), ('ព', 'n'), ('ិ', 'ns'), ('ធ', 'ns'), ('ី', 'ns'), ('ប', 'v'), ('្', 'ns'), ('រ', 'ns'), ('ក', 'ns'), ('ួ', 'ns'), ('ត', 'ns'), ('ព', 'n'), ('ា', 'ns'), ('ន', 'ns'), ('រ', 'n'), ('ង', 'ns'), ('្', 'ns'), ('វ', 'ns'), ('ា', 'ns'), ('ន', 'ns'), ('់', 'ns'), ('ព', 'n'), ('ិ', 'ns'), ('ភ', 'ns'), ('ព', 'ns'), ('ល', 'n'), ('ោ', 'ns'), ('ក', 'ns'), ('ន', 'o'), ('ៃ', 'ns'), ('ក', 'n'), ('ី', 'ns'), ('ឡ', 'ns'), ('ា', 'ns'), ('ប', 'n'), ('ា', 'ns'), ('ល', 'ns'), ('់', 'ns'), ('ឱ', 'v'), ('ប', 'ns'), ('ឆ', 'n'), ('្', 'ns'), ('ន', 'ns'), ('ា', 'ns'), ('ំ', 'ns'), ('2', '1'), ('0', 'ns'), ('0', 'ns'), ('7', 'ns'), ('ដ', 'n'), ('ែ', 'ns'), ('ល', 'ns'), ('ប', 'v'), ('្', 'ns'), ('រ', 'ns'), ('ព', 'ns'), ('្', 'ns'), ('រ', 'ns'), ('ឹ', 'ns'), ('ត', 'ns'), ('្', 'ns'), ('ត', 'ns'), ('ន', 'v-'), ('ៅ', 'ns'), ('ប', 'n'), ('៉', 'ns'), ('ា', 'ns'), ('ស', 'ns'), ('ឌ', 'n'), ('េ', 'ns'), ('ស', 'ns'), ('ប', 'n'), ('្', 'ns'), ('រ', 'ns'), ('ី', 'ns'), ('ន', 'ns'), ('ក', 'n'), ('្', 'ns'), ('រ', 'ns'), ('ុ', 'ns'), ('ង', 'ns'), ('ប', 'n'), ('៉', 'ns'), ('ា', 'ns'), ('រ', 'ns'), ('ី', 'ns'), ('ស', 'ns'), ('ប', 'n'), ('ា', 'ns'), ('រ', 'ns'), ('ា', 'ns'), ('ំ', 'ns'), ('ង', 'ns'), ('។', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(gen_char_with_label(tok_clean[0], tag_clean[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSi0D1OVpfmn"
      },
      "outputs": [],
      "source": [
        "def sentence_with_compound(sentence, tag):\n",
        "  sentence_with_compounds = []\n",
        "  start = 0\n",
        "  end = 0\n",
        "  pos_com = []\n",
        "  for i, j in enumerate(tag):\n",
        "    if j.find(\"[\") !=-1:\n",
        "      start = i\n",
        "      if tag[start+1].find(\"]\") !=-1:\n",
        "        end = start+2\n",
        "      elif tag[start+2].find(\"]\") !=-1:\n",
        "        end = start+3\n",
        "      elif tag[start+3].find(\"]\") !=-1:\n",
        "        end = start+4\n",
        "      compound = ''.join(sentence[start:end])\n",
        "      pos_com.append('c'+compound_tag(j))\n",
        "      sentence_with_compounds.append(compound)\n",
        "    if i not in range(start,end):\n",
        "        sentence_with_compounds.append(sentence[i])\n",
        "        pos_com.append(j)\n",
        "        # pos_com.append('cns')\n",
        "  return sentence_with_compounds, pos_com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQBSPp9y1E7D"
      },
      "outputs": [],
      "source": [
        "sentence_with_compounds = [sentence_with_compound(sentence, tags) for i, (sentence, tags) in enumerate(zip(tok_clean, tag_clean))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwgWTBYioRcf",
        "outputId": "cb50d518-6e51-4d1d-fd47-c4f51f514c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['អ៊ីតាលី', 'បាន', 'ឈ្នះ', 'លើ', 'ព័រទុយហ្គាល់', '31-5', 'ក្នុង', 'ប៉ូលC', 'នៃ', 'ពិធីប្រកួត', 'ពានរង្វាន់', 'ពិភពលោក', 'នៃ', 'កីឡាបាល់ឱប', 'ឆ្នាំ2007', 'ដែល', 'ប្រព្រឹត្ត', 'នៅ', 'ប៉ាសឌេសប្រីន', 'ក្រុងប៉ារីស', 'បារាំង', '។']\n",
            "['n', 'v-', 'v', 'o', 'n', 'n', 'o', 'cn', 'o', 'cn', 'cn', 'cn', 'o', 'cn', 'cn', 'n', 'v', 'v-', 'cn', 'cn', 'n', '.']\n",
            "19\n",
            "['ញូវ', 'សាឡេន', 'បាន', 'នាំ', 'មុខ', 'នៅ', 'ក្នុង', 'ក្រុម', 'មាន', '១០', 'ពិន្ទុ', 'ឈរ', 'លើ', 'ស្កុតឡេន', 'ដោយសារ', 'ពិន្ទុ', 'ខុស', 'គ្នា', '។']\n",
            "['ញូវ_n[n', 'សាឡេន_n]n', 'បាន_v-', 'នាំ_v[v', 'មុខ_n]v', 'នៅ_v-', 'ក្នុង_v-', 'ក្រុម_n', 'មាន_v', '១០_a[1', 'ពិន្ទុ_n]a', 'ឈរ_v', 'លើ_o', 'ស្កុតឡេន_n', 'ដោយសារ_o', 'ពិន្ទុ_n', 'ខុស_a[a', 'គ្នា_n]a', '។_.']\n"
          ]
        }
      ],
      "source": [
        "print(sentence_with_compounds[0][0])\n",
        "print(sentence_with_compounds[0][1])\n",
        "print(len(tok_clean[7]))\n",
        "print(tok_clean[7])\n",
        "print(data[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN5oX56TIcpd",
        "outputId": "f0df060a-eb79-4dcf-c742-6ae8b87141b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['អ៊ីតាលី_n', 'បាន_v-', 'ឈ្នះ_v', 'លើ_o', 'ព័រទុយហ្គាល់_n', '31-5_n', 'ក្នុង_o', 'ប៉ូល_n[n', 'C_n]n', 'នៃ_o', 'ពិធី_n[n', 'ប្រកួត_v]n', 'ពាន_n[n', 'រង្វាន់_n]n', 'ពិភព_n[n', 'លោក_n]n', 'នៃ_o', 'កីឡា_n[n', 'បាល់_n', 'ឱប_v]n', 'ឆ្នាំ_n[n', '2007_1]n', 'ដែល_n', 'ប្រព្រឹត្ត_v', 'នៅ_v-', 'ប៉ាស_n[n', 'ឌេស_n', 'ប្រីន_n]n', 'ក្រុង_n[n', 'ប៉ារីស_n]n', 'បារាំង_n', '។_.']\n",
            "['អ៊ីតាលី', 'បាន', 'ឈ្នះ', 'លើ', 'ព័រទុយហ្គាល់', '31-5', 'ក្នុង', 'ប៉ូល', 'C', 'នៃ', 'ពិធី', 'ប្រកួត', 'ពាន', 'រង្វាន់', 'ពិភព', 'លោក', 'នៃ', 'កីឡា', 'បាល់', 'ឱប', 'ឆ្នាំ', '2007', 'ដែល', 'ប្រព្រឹត្ត', 'នៅ', 'ប៉ាស', 'ឌេស', 'ប្រីន', 'ក្រុង', 'ប៉ារីស', 'បារាំង', '។']\n",
            "['n', 'v-', 'v', 'o', 'n', 'n', 'o', 'n[n', 'n]n', 'o', 'n[n', 'v]n', 'n[n', 'n]n', 'n[n', 'n]n', 'o', 'n[n', 'n', 'v]n', 'n[n', '1]n', 'n', 'v', 'v-', 'n[n', 'n', 'n]n', 'n[n', 'n]n', 'n', '.']\n",
            "[('អ', 'n'), ('៊', 'ns'), ('ី', 'ns'), ('ត', 'ns'), ('ា', 'ns'), ('ល', 'ns'), ('ី', 'ns'), ('ប', 'v-'), ('ា', 'ns'), ('ន', 'ns'), ('ឈ', 'v'), ('្', 'ns'), ('ន', 'ns'), ('ះ', 'ns'), ('ល', 'o'), ('ើ', 'ns'), ('ព', 'n'), ('័', 'ns'), ('រ', 'ns'), ('ទ', 'ns'), ('ុ', 'ns'), ('យ', 'ns'), ('ហ', 'ns'), ('្', 'ns'), ('គ', 'ns'), ('ា', 'ns'), ('ល', 'ns'), ('់', 'ns'), ('3', 'n'), ('1', 'ns'), ('-', 'ns'), ('5', 'ns'), ('ក', 'o'), ('្', 'ns'), ('ន', 'ns'), ('ុ', 'ns'), ('ង', 'ns'), ('ប', 'n'), ('៉', 'ns'), ('ូ', 'ns'), ('ល', 'ns'), ('C', 'n'), ('ន', 'o'), ('ៃ', 'ns'), ('ព', 'n'), ('ិ', 'ns'), ('ធ', 'ns'), ('ី', 'ns'), ('ប', 'v'), ('្', 'ns'), ('រ', 'ns'), ('ក', 'ns'), ('ួ', 'ns'), ('ត', 'ns'), ('ព', 'n'), ('ា', 'ns'), ('ន', 'ns'), ('រ', 'n'), ('ង', 'ns'), ('្', 'ns'), ('វ', 'ns'), ('ា', 'ns'), ('ន', 'ns'), ('់', 'ns'), ('ព', 'n'), ('ិ', 'ns'), ('ភ', 'ns'), ('ព', 'ns'), ('ល', 'n'), ('ោ', 'ns'), ('ក', 'ns'), ('ន', 'o'), ('ៃ', 'ns'), ('ក', 'n'), ('ី', 'ns'), ('ឡ', 'ns'), ('ា', 'ns'), ('ប', 'n'), ('ា', 'ns'), ('ល', 'ns'), ('់', 'ns'), ('ឱ', 'v'), ('ប', 'ns'), ('ឆ', 'n'), ('្', 'ns'), ('ន', 'ns'), ('ា', 'ns'), ('ំ', 'ns'), ('2', '1'), ('0', 'ns'), ('0', 'ns'), ('7', 'ns'), ('ដ', 'n'), ('ែ', 'ns'), ('ល', 'ns'), ('ប', 'v'), ('្', 'ns'), ('រ', 'ns'), ('ព', 'ns'), ('្', 'ns'), ('រ', 'ns'), ('ឹ', 'ns'), ('ត', 'ns'), ('្', 'ns'), ('ត', 'ns'), ('ន', 'v-'), ('ៅ', 'ns'), ('ប', 'n'), ('៉', 'ns'), ('ា', 'ns'), ('ស', 'ns'), ('ឌ', 'n'), ('េ', 'ns'), ('ស', 'ns'), ('ប', 'n'), ('្', 'ns'), ('រ', 'ns'), ('ី', 'ns'), ('ន', 'ns'), ('ក', 'n'), ('្', 'ns'), ('រ', 'ns'), ('ុ', 'ns'), ('ង', 'ns'), ('ប', 'n'), ('៉', 'ns'), ('ា', 'ns'), ('រ', 'ns'), ('ី', 'ns'), ('ស', 'ns'), ('ប', 'n'), ('ា', 'ns'), ('រ', 'ns'), ('ា', 'ns'), ('ំ', 'ns'), ('ង', 'ns'), ('។', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(data[0])\n",
        "print(tok_clean[0])\n",
        "print(tag_clean[0])\n",
        "print(gen_char_with_label(tok_clean[0], tag_clean[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1yeiSkYMlzt"
      },
      "source": [
        "## Separating characters and tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjcJU82NMu7n"
      },
      "outputs": [],
      "source": [
        "char_labels = [gen_char_with_label(sentence, tags) for sentence, tags in zip(tok_clean, tag_clean)]\n",
        "chars_only = [[x[0] for x in sent] for sent in char_labels]\n",
        "labels_only_short = [[tags2idx[x[1]] for x in sent] for sent in char_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjPLOYrnxYOk",
        "outputId": "ae8c08c1-3873-45cf-e0bc-bc5e28cadb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "584\n",
            "[2, 1, 1, 1, 1, 1, 1, 10, 1, 1, 3, 1, 1, 1, 5, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 1, 2, 1, 1, 1, 2, 5, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 5, 1, 2, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 7]\n"
          ]
        }
      ],
      "source": [
        "leng = []\n",
        "for i in char_labels:\n",
        "  leng.append(len(i))\n",
        "print(max(leng))\n",
        "\n",
        "print(labels_only_short[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bmA7T1lolSb"
      },
      "outputs": [],
      "source": [
        "tag_compounds = [sentence[1] for sentence in sentence_with_compounds]\n",
        "char_labels_compounds = [gen_char_with_label_compound(sentence[0], sentence[1]) for sentence in sentence_with_compounds]\n",
        "chars_only_compounds = [[x[0] for x in sent] for sent in char_labels_compounds]\n",
        "labels_only_compounds = [[tags2idx[x[1]] for x in sent] for sent in char_labels_compounds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m24MQhohM3JX",
        "outputId": "1f1f1142-c76d-4421-8d2d-622f6513e4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['អ៊ីតាលី', 'បាន', 'ឈ្នះ', 'លើ', 'ព័រទុយហ្គាល់', '31-5', 'ក្នុង', 'ប៉ូលC', 'នៃ', 'ពិធីប្រកួត', 'ពានរង្វាន់', 'ពិភពលោក', 'នៃ', 'កីឡាបាល់ឱប', 'ឆ្នាំ2007', 'ដែល', 'ប្រព្រឹត្ត', 'នៅ', 'ប៉ាសឌេសប្រីន', 'ក្រុងប៉ារីស', 'បារាំង', '។']\n",
            "['n', 'v-', 'v', 'o', 'n', 'n', 'o', 'cn', 'o', 'cn', 'cn', 'cn', 'o', 'cn', 'cn', 'n', 'v', 'v-', 'cn', 'cn', 'n', '.']\n",
            "[('អ', 'n'), ('៊', 'cns'), ('ី', 'cns'), ('ត', 'cns'), ('ា', 'cns'), ('ល', 'cns'), ('ី', 'cns'), ('ប', 'v-'), ('ា', 'cns'), ('ន', 'cns'), ('ឈ', 'v'), ('្', 'cns'), ('ន', 'cns'), ('ះ', 'cns'), ('ល', 'o'), ('ើ', 'cns'), ('ព', 'n'), ('័', 'cns'), ('រ', 'cns'), ('ទ', 'cns'), ('ុ', 'cns'), ('យ', 'cns'), ('ហ', 'cns'), ('្', 'cns'), ('គ', 'cns'), ('ា', 'cns'), ('ល', 'cns'), ('់', 'cns'), ('3', 'n'), ('1', 'cns'), ('-', 'cns'), ('5', 'cns'), ('ក', 'o'), ('្', 'cns'), ('ន', 'cns'), ('ុ', 'cns'), ('ង', 'cns'), ('ប', 'cn'), ('៉', 'cns'), ('ូ', 'cns'), ('ល', 'cns'), ('C', 'cns'), ('ន', 'o'), ('ៃ', 'cns'), ('ព', 'cn'), ('ិ', 'cns'), ('ធ', 'cns'), ('ី', 'cns'), ('ប', 'cns'), ('្', 'cns'), ('រ', 'cns'), ('ក', 'cns'), ('ួ', 'cns'), ('ត', 'cns'), ('ព', 'cn'), ('ា', 'cns'), ('ន', 'cns'), ('រ', 'cns'), ('ង', 'cns'), ('្', 'cns'), ('វ', 'cns'), ('ា', 'cns'), ('ន', 'cns'), ('់', 'cns'), ('ព', 'cn'), ('ិ', 'cns'), ('ភ', 'cns'), ('ព', 'cns'), ('ល', 'cns'), ('ោ', 'cns'), ('ក', 'cns'), ('ន', 'o'), ('ៃ', 'cns'), ('ក', 'cn'), ('ី', 'cns'), ('ឡ', 'cns'), ('ា', 'cns'), ('ប', 'cns'), ('ា', 'cns'), ('ល', 'cns'), ('់', 'cns'), ('ឱ', 'cns'), ('ប', 'cns'), ('ឆ', 'cn'), ('្', 'cns'), ('ន', 'cns'), ('ា', 'cns'), ('ំ', 'cns'), ('2', 'cns'), ('0', 'cns'), ('0', 'cns'), ('7', 'cns'), ('ដ', 'n'), ('ែ', 'cns'), ('ល', 'cns'), ('ប', 'v'), ('្', 'cns'), ('រ', 'cns'), ('ព', 'cns'), ('្', 'cns'), ('រ', 'cns'), ('ឹ', 'cns'), ('ត', 'cns'), ('្', 'cns'), ('ត', 'cns'), ('ន', 'v-'), ('ៅ', 'cns'), ('ប', 'cn'), ('៉', 'cns'), ('ា', 'cns'), ('ស', 'cns'), ('ឌ', 'cns'), ('េ', 'cns'), ('ស', 'cns'), ('ប', 'cns'), ('្', 'cns'), ('រ', 'cns'), ('ី', 'cns'), ('ន', 'cns'), ('ក', 'cn'), ('្', 'cns'), ('រ', 'cns'), ('ុ', 'cns'), ('ង', 'cns'), ('ប', 'cns'), ('៉', 'cns'), ('ា', 'cns'), ('រ', 'cns'), ('ី', 'cns'), ('ស', 'cns'), ('ប', 'n'), ('ា', 'cns'), ('រ', 'cns'), ('ា', 'cns'), ('ំ', 'cns'), ('ង', 'cns'), ('។', '.')]\n",
            "['អ', '៊', 'ី', 'ត', 'ា', 'ល', 'ី', 'ប', 'ា', 'ន', 'ឈ', '្', 'ន', 'ះ', 'ល', 'ើ', 'ព', '័', 'រ', 'ទ', 'ុ', 'យ', 'ហ', '្', 'គ', 'ា', 'ល', '់', '3', '1', '-', '5', 'ក', '្', 'ន', 'ុ', 'ង', 'ប', '៉', 'ូ', 'ល', 'C', 'ន', 'ៃ', 'ព', 'ិ', 'ធ', 'ី', 'ប', '្', 'រ', 'ក', 'ួ', 'ត', 'ព', 'ា', 'ន', 'រ', 'ង', '្', 'វ', 'ា', 'ន', '់', 'ព', 'ិ', 'ភ', 'ព', 'ល', 'ោ', 'ក', 'ន', 'ៃ', 'ក', 'ី', 'ឡ', 'ា', 'ប', 'ា', 'ល', '់', 'ឱ', 'ប', 'ឆ', '្', 'ន', 'ា', 'ំ', '2', '0', '0', '7', 'ដ', 'ែ', 'ល', 'ប', '្', 'រ', 'ព', '្', 'រ', 'ឹ', 'ត', '្', 'ត', 'ន', 'ៅ', 'ប', '៉', 'ា', 'ស', 'ឌ', 'េ', 'ស', 'ប', '្', 'រ', 'ី', 'ន', 'ក', '្', 'រ', 'ុ', 'ង', 'ប', '៉', 'ា', 'រ', 'ី', 'ស', 'ប', 'ា', 'រ', 'ា', 'ំ', 'ង', '។']\n",
            "[2, 13, 13, 13, 13, 13, 13, 10, 13, 13, 3, 13, 13, 13, 5, 13, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 2, 13, 13, 13, 5, 13, 13, 13, 13, 14, 13, 13, 13, 13, 5, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 5, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 2, 13, 13, 3, 13, 13, 13, 13, 13, 13, 13, 13, 13, 10, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 2, 13, 13, 13, 13, 13, 7]\n"
          ]
        }
      ],
      "source": [
        "print(sentence_with_compounds[0][0])\n",
        "print(sentence_with_compounds[0][1])\n",
        "print(char_labels_compounds[0])\n",
        "print(chars_only_compounds[0])\n",
        "print(labels_only_compounds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVSKOByJ0QVi"
      },
      "source": [
        "## Defining compounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCPy5JyPM8vS"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx5MiFGNNBrz"
      },
      "outputs": [],
      "source": [
        "def pad_input(sents, seq_len, isFeature = True):\n",
        "    features = np.zeros((len(sents), seq_len),dtype=int)\n",
        "    if isFeature == False:\n",
        "        features +=-1\n",
        "    for ii, review in enumerate(sents):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfEMXDCACcm4"
      },
      "outputs": [],
      "source": [
        "# pad_mor = pad_input(labels_only_short, seq_len=300)\n",
        "# pad_com = pad_input(labels_only_compounds, seq_len=300)\n",
        "# y_labels = [[labels_mor, labels_com] for labels_mor, labels_com in zip(pad_mor,pad_com)]\n",
        "# print(y_labels[0])\n",
        "# both_labels_final = []\n",
        "# both_labels = []\n",
        "# for labels in y_labels:\n",
        "#   both_labels = [[i, j] for i, j in zip(labels[0], labels[1])]\n",
        "#   both_labels_final.append(both_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ih8ZiZYpxMX",
        "outputId": "f04acd17-614c-4cb7-a737-9f169896f6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  1  1  1  1\n",
            "  1  1 10  1  1  3  1  1  1  5  1  2  1  1  1  1  1  1  1  1  1  1  1  2\n",
            "  1  1  1  5  1  1  1  1  2  1  1  1  2  5  1  2  1  1  1  3  1  1  1  1\n",
            "  1  2  1  1  2  1  1  1  1  1  1  2  1  1  1  2  1  1  5  1  2  1  1  1\n",
            "  2  1  1  1  3  1  2  1  1  1  1  6  1  1  1  2  1  1  3  1  1  1  1  1\n",
            "  1  1  1  1 10  1  2  1  1  1  2  1  1  2  1  1  1  1  2  1  1  1  1  2\n",
            "  1  1  1  1  1  2  1  1  1  1  1  7]\n",
            "[array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "        1, 12,  1,  1,  3,  1,  3,  1,  5,  1,  1, 12,  1,  1,  2,  1,  1,\n",
            "        1,  1,  2,  1,  1,  1,  1,  1,  1,  1, 10,  1,  1,  3,  1,  1,  1,\n",
            "       10,  1,  1,  5,  1,  1,  1,  1,  2,  1,  1,  1,  2,  1,  6,  1,  2,\n",
            "        1,  1,  3,  1,  2,  1,  1,  1,  1,  1,  5,  1,  6,  1,  1,  4,  1,\n",
            "        1,  5,  1,  1,  1,  2,  1,  1,  9,  1,  5,  1,  1,  1,  1,  2,  1,\n",
            "        1,  3,  1,  1,  1,  1,  1, 11,  1,  1,  7]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "        0,  0,  0,  0,  0,  0,  2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "       13, 12, 13, 13, 15, 13, 13, 13,  5, 13, 13, 12, 13, 13, 14, 13, 13,\n",
            "       13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 10, 13, 13,  3, 13, 13, 13,\n",
            "       10, 13, 13,  5, 13, 13, 13, 13,  2, 13, 13, 13, 14, 13, 13, 13,  2,\n",
            "       13, 13,  3, 13,  2, 13, 13, 13, 13, 13, 17, 13, 13, 13, 13, 13, 13,\n",
            "       13,  5, 13, 13, 13, 14, 13, 13, 13, 13,  5, 13, 13, 13, 13, 14, 13,\n",
            "       13, 13, 13, 13, 13, 13, 13, 11, 13, 13,  7])]\n",
            "[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [2, 2], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [10, 10], [1, 13], [1, 13], [3, 3], [1, 13], [1, 13], [1, 13], [5, 5], [1, 13], [2, 2], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 2], [1, 13], [1, 13], [1, 13], [5, 5], [1, 13], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [2, 13], [5, 5], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [3, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [5, 5], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [1, 13], [3, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [1, 13], [6, 13], [1, 13], [1, 13], [1, 13], [2, 2], [1, 13], [1, 13], [3, 3], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [10, 10], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 2], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [7, 7]]\n"
          ]
        }
      ],
      "source": [
        "pad_mor = pad_input(labels_only_short, seq_len=300)\n",
        "print(pad_mor[0])\n",
        "pad_com = pad_input(labels_only_compounds, seq_len=300)\n",
        "y_labels = [[labels_mor, labels_com] for labels_mor, labels_com in zip(pad_mor,pad_com)]\n",
        "print(y_labels[3])\n",
        "both_labels_final = []\n",
        "both_labels = []\n",
        "for labels in y_labels:\n",
        "  both_labels = [[i, j] for i, j in zip(labels[0], labels[1])]\n",
        "  both_labels_final.append(both_labels)\n",
        "print(both_labels_final[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWqf_xVtwZxq"
      },
      "source": [
        "## One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtDgw2MZwYp9"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    arr = arr.numpy()\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return torch.from_numpy(one_hot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0twgDWGowi-A"
      },
      "source": [
        "## Train and Test Set Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhGO7HYG9jUY"
      },
      "outputs": [],
      "source": [
        "def split_data(X_char, y_char,tags2idx,chars2idx,sentence_length=100):\n",
        "    X_train_char, X_test_char, y_train_char, y_test_char = train_test_split(X_char, y_char, test_size=0.20, random_state=1)\n",
        "\n",
        "    for i, sentence in enumerate(X_train_char):\n",
        "        # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "        X_train_char[i] = [chars2idx[c] if c in chars2idx else 1 for c in sentence]\n",
        "    # for i, sentence in enumerate(y_train_char):\n",
        "    # #   # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    #     y_train_char[i] = [tags2idx[c] if c in tags2idx else 1 for c in sentence]\n",
        "    for i, sentence in enumerate(X_test_char):\n",
        "        # For test sentences, we have to tokenize the sentences as well\n",
        "        X_test_char[i] = [chars2idx[c] if c in chars2idx else 1 for c in sentence]\n",
        "    # for i, sentence in enumerate(y_test_char):\n",
        "    #   # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    #     y_test_char[i] = [tags2idx[c] if c in tags2idx else 1 for c in sentence]\n",
        "\n",
        "    X_train_char = pad_input(X_train_char,sentence_length)\n",
        "    X_test_char = pad_input(X_test_char,sentence_length)\n",
        "    # y_train_char_mor = pad_input(y_train_char[0],sentence_length,False)\n",
        "    # y_train_char_com = pad_input(y_train_char[1],sentence_length,False)\n",
        "    # y_train_char.append(y_train_char_mor)\n",
        "    # y_train_char.append(y_train_char_com)\n",
        "    # y_test_char_mor = pad_input(y_test_char[0],sentence_length,False)\n",
        "    # print(y_test_char_mor )\n",
        "    # y_test_char_com = pad_input(y_test_char[1],sentence_length,False)\n",
        "    # y_test_char.append(y_test_char_mor)\n",
        "    # y_test_char.append(y_test_char_com)\n",
        "    return X_train_char, X_test_char, y_train_char, y_test_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5K8JGVsnSqI",
        "outputId": "3bfcf159-b204-4f55-e56d-756b8f5cd3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20106\n"
          ]
        }
      ],
      "source": [
        "print(len(both_labels_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5jhWC3Qw1BT"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = split_data(chars_only , both_labels_final,tags2idx,chars2idx,sentence_length=300)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB9El4nWK7nM",
        "outputId": "1d46579e-0ccd-491f-863f-090af2d3a6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [2, 2], [1, 13], [1, 13], [3, 3], [1, 13], [1, 13], [1, 13], [1, 13], [5, 5], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [3, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [5, 5], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [3, 15], [1, 13], [1, 13], [3, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 2], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [3, 3], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [1, 13], [4, 13], [1, 13], [1, 13], [5, 5], [1, 13], [1, 13], [3, 3], [1, 13], [1, 13], [1, 13], [1, 13], [5, 5], [1, 13], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [3, 13], [1, 13], [1, 13], [3, 13], [1, 13], [1, 13], [5, 17], [1, 13], [1, 13], [5, 13], [1, 13], [1, 13], [10, 10], [1, 13], [1, 13], [1, 13], [1, 13], [3, 3], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [5, 5], [1, 13], [1, 13], [1, 13], [1, 13], [2, 14], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 2], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [2, 2], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [1, 13], [7, 7]]\n",
            "16084\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  1  3 74 11 60 71 22 55 21  9 55 22 77 24 21 74\n",
            " 20 21 55  4 55 28 24 66 11 26 62 27  8 74 21 55 71 21 56  6 33 60  3 25\n",
            " 55 24 28 22 33 77  3 74 11 60 71 17 74 28 61 30 22 55 21 20 74 29 55  2\n",
            " 77  7 60 72 12 60 21 12 55 22 85  1]\n"
          ]
        }
      ],
      "source": [
        "print((y_train[6]))\n",
        "print(len(y_train))\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1gTKrWpzxi2",
        "outputId": "a36c590f-42a8-4f8a-c79f-9341179ef8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
          ]
        }
      ],
      "source": [
        "# POS_IDX = list((-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22))\n",
        "POS_IDX = [ i[0] for i in enumerate(POSTAGS) ]\n",
        "print(POS_IDX)\n",
        "label_one_hot_train = []\n",
        "label_one_hot_test = []\n",
        "for i in range(len(y_train)):\n",
        "  labels_array_train = np.asarray(y_train[i])\n",
        "# print(labels_array)\n",
        "  one_hot = MultiLabelBinarizer(classes = POS_IDX)\n",
        "  label_one_hot = np.array(one_hot.fit_transform(labels_array_train))\n",
        "  #label_one_hot.resize(300, len(POSTAGS))\n",
        "  label_one_hot_train.append(label_one_hot)\n",
        "for i in range(len(y_test)):\n",
        "  labels_array_test = np.asarray(y_test[i])\n",
        "# print(labels_array)\n",
        "  one_hot_1 = MultiLabelBinarizer(classes = POS_IDX)\n",
        "  label_one_hot_1 = np.array(one_hot_1.fit_transform(labels_array_test))\n",
        "  #label_one_hot_1.resize(300, len(POSTAGS))\n",
        "  label_one_hot_test.append(label_one_hot_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oouwuw2jWWAB",
        "outputId": "bd374c83-0c3e-4dce-c00a-491fe6244c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "16084\n"
          ]
        }
      ],
      "source": [
        "print(label_one_hot_train [0][200:230])\n",
        "print(len(label_one_hot_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKPM9Fhy9AxJ",
        "outputId": "1a5ee14e-6f17-4869-b091-89ccafa912b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "# label_one_hot_train = pad_input(label_one_hot_train, seq_len=300, isFeature = False)\n",
        "# label_one_hot_test = pad_input(label_one_hot_test, seq_len=300, isFeature = False)\n",
        "y_train_tensor = torch.tensor(label_one_hot_train)\n",
        "y_test_tensor = torch.tensor(label_one_hot_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SA4ux1Wwuhz"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaItWa5OwuAc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# #a/ data loader\n",
        "batch_size = 64\n",
        "# create your dataset\n",
        "train_dataset = TensorDataset(torch.tensor(X_train).long(),y_train_tensor.long()) \n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "# create your dataset\n",
        "test_dataset = TensorDataset(torch.tensor(X_test).long(),y_test_tensor.long()) \n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLwNv5laxEvd",
        "outputId": "e6857294-354b-4f9f-b86e-4c1e2b617754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input size:  torch.Size([64, 300])\n",
            "Sample input: \n",
            " tensor([[ 0,  0,  0,  ..., 27, 85,  1],\n",
            "        [ 0,  0,  0,  ..., 63, 27, 85],\n",
            "        [ 0,  0,  0,  ..., 69, 72, 85],\n",
            "        ...,\n",
            "        [ 0,  0,  0,  ..., 69, 72, 85],\n",
            "        [ 0,  0,  0,  ..., 67, 28, 85],\n",
            "        [ 0,  0,  0,  ..., 60, 72, 85]])\n",
            "\n",
            "Sample label size:  torch.Size([64, 300, 25])\n",
            "Sample label: \n",
            " tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 1, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]])\n"
          ]
        }
      ],
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_dl)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIvpGK76EbLX"
      },
      "outputs": [],
      "source": [
        "cc = one_hot_encode(torch.tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   1,  30,  56,   9,  56,\n",
        "         22,  75,  28,  36,  22,  78,  20,  26,  75,  26,  17,  56,   9,  56,\n",
        "         20,  61,   6, 163, 158, 161,  12,  68,  29,  26,  56,  21,  19,  72,\n",
        "         34,  72, 108, 115, 125, 130, 119, 124, 121, 135,  19,  19,  59,   6,\n",
        "         22,  75,  28,  34,  68,  29,  26,  63,  27, 122, 131, 131, 136,   2,\n",
        "         21,  75,  29,  73,  86,   1]), len(CHARS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNxfqUMCZgqr",
        "outputId": "5a41e94e-8b22-418d-f033-2d18db47c681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "print(cc[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR3Fh-JL_BUX"
      },
      "outputs": [],
      "source": [
        "cc = one_hot_encode(torch.tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   1,  30,  56,   9,  56,\n",
        "         22,  75,  28,  36,  22,  78,  20,  26,  75,  26,  17,  56,   9,  56,\n",
        "         20,  61,   6, 163, 158, 161,  12,  68,  29,  26,  56,  21,  19,  72,\n",
        "         34,  72, 108, 115, 125, 130, 119, 124, 121, 135,  19,  19,  59,   6,\n",
        "         22,  75,  28,  34,  68,  29,  26,  63,  27, 122, 131, 131, 136,   2,\n",
        "         21,  75,  29,  73,  86,   1]), len(CHARS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo7gh0iUTH4b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-huwiF0TIAr",
        "outputId": "b262b57f-9ee2-4e03-ffdf-262520b4d039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "print(cc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7F5bdEQYWu0",
        "outputId": "f8ea62c3-e4cc-4ac2-e422-532f1fce52cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "វ\n",
            "ិ\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "example = [ 30,  56]\n",
        "example_one_hot = one_hot_encode(torch.tensor([ -1,  -1]), len(POSTAGS))\n",
        "for i in example:\n",
        "  print(idx2chars[i])\n",
        "example_one_hot[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1l2iirAF7fj",
        "outputId": "5c770624-0cf1-4cb3-85dc-979683961c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "print(cc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy2HY2wgGg8D",
        "outputId": "62999c87-8e03-42ca-f3aa-62838d929706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "168\n"
          ]
        }
      ],
      "source": [
        "print(len(CHARS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvk6urbxEZ-"
      },
      "source": [
        "# LSTM Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD3_jSW_xVFi"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WordSegPosTag(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_input, n_output, n_hidden=100, n_layers=2,\n",
        "                               drop_prob=0.5): \n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        \n",
        "\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, n_output)\n",
        "        #self.sig = nn.Sigmoid()\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        #sig_out = self.sig(out)\n",
        "        \n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDLRfqjxbLW"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOvL9BmoL8p7"
      },
      "outputs": [],
      "source": [
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfyeSRAZmaYK"
      },
      "outputs": [],
      "source": [
        "val_losses = []\n",
        "losses = []\n",
        "losses_final=[]\n",
        "val_losses_final = []\n",
        "valid_accuracy = []\n",
        "training_accuracy = []\n",
        "def train(net, train_dl,test_dl,seq_length=300, epochs=100, lr=0.001, clip=5, print_every=100):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    train_correct = 0.\n",
        "    train_total = 0.\n",
        "    net.train()\n",
        "    start = time.time()\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    # criterion = nn.CrossEntropyLoss(ignore_index = -1)\n",
        "    weight = torch.tensor([0, 1, 1, 1,1 , 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1,1, 1, 1, 1, 1, 1]).cuda()\n",
        "    #pos_weight = torch.tensor([1,1, 1, 1, 1, 1, 1,1, 1, 1, 1 ,1 ,1 ,1, 1, 1, 1, 1, 1, 1, 1, 1, 1]).cuda()\n",
        "    criterion = nn.BCELoss(weight =weight)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    #n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "\n",
        "        \n",
        "        for x, y in train_dl:\n",
        "                    # initialize hidden state\n",
        "            batch_size = x.shape[0]\n",
        "            h = net.init_hidden(batch_size)\n",
        "            counter += 1\n",
        "            #print(x)\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, len(CHARS))\n",
        "            inputs, targets = x, y\n",
        "            if(train_on_gpu):\n",
        "                inputs, targets= inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            # # calculate the loss and perform backprop\n",
        "            output = torch.sigmoid(output)\n",
        "            predicted = torch.round(output)\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length,len(POSTAGS)).float())\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            train_total += targets.view(batch_size*seq_length,len(POSTAGS)).float().size(0)*len(POSTAGS)\n",
        "            #calculate how many images were correctly classified\n",
        "            train_correct += (predicted == targets.view(batch_size*seq_length,len(POSTAGS)).float()).sum().item()\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step() \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                net.eval()\n",
        "                for x, y in test_dl:\n",
        "                    batch_size = x.shape[0]\n",
        "                    val_h = net.init_hidden(batch_size)\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, len(CHARS))\n",
        "                    inputs, targets = x, y\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    output = torch.sigmoid(output)\n",
        "                    predicted = torch.round(output)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length,len(POSTAGS)).float())\n",
        "                    #print(targets.view(batch_size*seq_length).shape)\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    #accuracy_list.append(acc.item())\n",
        "                    total += targets.view(batch_size*seq_length,len(POSTAGS)).float().size(0)*len(POSTAGS)\n",
        "                     #calculate how many images were correctly classified\n",
        "                    correct += (predicted == targets.view(batch_size*seq_length,len(POSTAGS)).float()).sum().item()\n",
        "                #     print(\"predicted == targets.view(batch_size*seq_length,len(POSTAGS)).float()\", (predicted == targets.view(batch_size*seq_length,len(POSTAGS)).float()).sum())\n",
        "                #     print(\"predicted\", predicted.size(0))\n",
        "                #     print(\"targets\" , targets.view(batch_size*seq_length,len(POSTAGS)).float().size())\n",
        "                accuracy = 100 * correct / total\n",
        "                valid_accuracy.append(accuracy)\n",
        "                print(\"valid correction\", correct)\n",
        "                train_accuracy = 100 * train_correct / train_total\n",
        "                training_accuracy.append(train_accuracy) \n",
        "                print(\"train correction\", train_correct)\n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                losses_final.append(np.mean(losses))\n",
        "                val_losses_final.append(np.mean(val_losses))\n",
        "\n",
        "                print(\"Time: {}...\".format(time_since(start)),\n",
        "                      \"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                      \"Train Accuracy: {}%\".format(train_accuracy), \n",
        "                      \"Val Accuracy: {}%\".format(accuracy))\n",
        "                #\"Accuracy: {:.4f}\".format(np.mean(accuracy_list)),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_d157MHNUMP"
      },
      "outputs": [],
      "source": [
        "# m = nn.Sigmoid()\n",
        "# loss = nn.BCELoss()\n",
        "# input = torch.randn(3, requires_grad=True)\n",
        "# target = torch.empty(3).random_(2)\n",
        "# output = loss(m(input), target)\n",
        "# output.backward()\n",
        "# print(input)\n",
        "# print(target)\n",
        "# print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWFeBA6UxjC8",
        "outputId": "7db90c54-7293-443f-e473-89f9beaa29e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordSegPosTag(\n",
            "  (lstm): LSTM(168, 256, num_layers=5, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=25, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net = WordSegPosTag(len(CHARS),len(POSTAGS), n_layers = 5,n_hidden=256,drop_prob=0.5 ) #3 #512\n",
        "print(net)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt-C7bXOxthu",
        "outputId": "5fb9c0fd-4a86-4969-9119-d991b2b0c55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid correction 29188216.0\n",
            "train correction 46328846.0\n",
            "Time: 0m 17s... Epoch: 1/2... Step: 100... loss: 0.0438... Val loss: 0.0701 Train Accuracy: 96.51842916666666% Val Accuracy: 96.76186308635836%\n",
            "valid correction 58448591.0\n",
            "train correction 92737096.0\n",
            "Time: 0m 34s... Epoch: 1/2... Step: 200... loss: 0.0311... Val loss: 0.0636 Train Accuracy: 96.60114166666666% Val Accuracy: 96.88147024697497%\n",
            "valid correction 88110508.0\n",
            "train correction 139094165.0\n",
            "Time: 0m 52s... Epoch: 2/2... Step: 300... loss: 0.0226... Val loss: 0.0579 Train Accuracy: 96.81503793415466% Val Accuracy: 97.36505663296315%\n",
            "valid correction 117808301.0\n",
            "train correction 185863530.0\n",
            "Time: 1m 9s... Epoch: 2/2... Step: 400... loss: 0.0254... Val loss: 0.0535 Train Accuracy: 96.97059007669432% Val Accuracy: 97.63658296038456%\n",
            "valid correction 147518703.0\n",
            "train correction 232686546.0\n",
            "Time: 1m 26s... Epoch: 2/2... Step: 500... loss: 0.0231... Val loss: 0.0499 Train Accuracy: 97.08622105394917% Val Accuracy: 97.807858776728%\n"
          ]
        }
      ],
      "source": [
        "model_train = train(net,train_dl,test_dl, epochs=2, lr=0.001, clip=5, print_every=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_A_0K8AWfQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ec5698-6f12-45d2-f601-983f01b76b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4142, 0.7034, 0.9794, 0.9718, 0.0139, 0.4853, 0.7171, 0.1597, 0.6440,\n",
            "        0.7804, 0.7912, 0.2911, 0.1118, 0.5814, 0.6843, 0.5417, 0.5226, 0.7133,\n",
            "        0.6949, 0.7449, 0.9237, 0.0802, 0.3818, 0.9130])\n"
          ]
        }
      ],
      "source": [
        "print(torch.rand(24))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNM4YCOQ2eXm"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(model_train, open(\"model_gb_v1.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYq24ZsbGKJJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses_final,'-o')\n",
        "plt.plot(val_losses_final,'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Train','Valid'])\n",
        "plt.title('Train vs Valid Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioUVvzTCVWuI"
      },
      "outputs": [],
      "source": [
        "print(valid_accuracy)\n",
        "print(training_accuracy)\n",
        "plt.plot(training_accuracy,'-o')\n",
        "plt.plot(valid_accuracy,'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Train','Valid'])\n",
        "plt.title('Train vs Valid Accuracy')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-hq82sd3suj"
      },
      "outputs": [],
      "source": [
        "app = FastAPI(title=\"ML Models as API on Google Colab\", description=\"with FastAPI and ColabCode\", version=\"1.0\")\n",
        "@app.on_event(\"startup\")\n",
        "@app.post(\"/api\", tags=[\"prediction\"])\n",
        "def load_model():\n",
        "    global model\n",
        "    model = pickle.load(open(\"model_gb.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLNlr4wpBMAB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OueWKQ-kBGQV"
      },
      "outputs": [],
      "source": [
        "# plt.plot(loss,'-o')\n",
        "# plt.plot(val_losses,'-o')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.legend(['Train','Valid'])\n",
        "# plt.title('Train vs Valid Accuracy')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EYoFQTt65zI",
        "outputId": "25322f0d-b225-478c-c34e-4865a9429347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordSegPosTag(\n",
            "  (lstm): LSTM(168, 256, num_layers=5, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=25, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqlzT_3ePVA"
      },
      "outputs": [],
      "source": [
        "# # change the name, for saving multiple files\n",
        "# model_name = 'rnn_1_epoch.net'\n",
        "\n",
        "# checkpoint = {'n_hidden': net.n_hidden,\n",
        "#               'n_layers': net.n_layers,\n",
        "#               'state_dict': net.state_dict()}\n",
        "\n",
        "# with open(model_name, 'wb') as f:\n",
        "#     torch.save(checkpoint, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcWAfGOafC4q"
      },
      "outputs": [],
      "source": [
        "# Here we have loaded in a model that trained over 1 epoch `rnn_1_epoch.net`\n",
        "# with open('rnn_1_epoch.net', 'rb') as f:\n",
        "#     checkpoint = torch.load(f)\n",
        "    \n",
        "# loaded = WordSegPosTag(len(CHARS), len(POSTAGS), n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "# loaded.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrCMWGccf_r_"
      },
      "outputs": [],
      "source": [
        "# if torch.cuda.is_available():\n",
        "#    loaded.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a path\n",
        "PATH = \"/content/drive/MyDrive/Dataset/Khmer NLP/entire_model.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(net, PATH)\n",
        "\n",
        "# # Load\n",
        "# model = torch.load(PATH)\n",
        "# model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHZJgFOp_mKZ",
        "outputId": "a02e75da-a4b9-4b43-8f17-9c4ef9fbe079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordSegPosTag(\n",
              "  (lstm): LSTM(168, 256, num_layers=5, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=25, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLHgfMvwxizb"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o_JFxuSMIXR"
      },
      "outputs": [],
      "source": [
        "# def segment(sample,net):\n",
        "#     net.eval()\n",
        "#     #sample ='ខ្ញុំទៅសាលារៀន'\n",
        "#     sample_chars = list(sample)\n",
        "#     sample_index = [chars2idx[x] for x in sample_chars ]\n",
        "#     sample_arr = torch.from_numpy(np.array(sample_index)).unsqueeze(0)\n",
        "#     sample_encoded = one_hot_encode(sample_arr,len(CHARS))\n",
        "\n",
        "#     if(train_on_gpu):\n",
        "#         sample_encoded = sample_encoded.cuda()\n",
        "\n",
        "#     h = net.init_hidden(1)\n",
        "\n",
        "#     h = tuple([each.data for each in h])\n",
        "#     outputs,_ = net(sample_encoded,h)\n",
        "#     outputs = torch.sigmoid(outputs)\n",
        "#     if(train_on_gpu):\n",
        "#         outputs  =outputs.detach().cpu().numpy()\n",
        "#     else:\n",
        "#         outputs  =outputs.detach().numpy()\n",
        "#     print(outputs)\n",
        "#     # idx = np.argmax(outputs, axis = 1)\n",
        "#     # print(idx)\n",
        "#     result = ''\n",
        "#     t = ''\n",
        "#     print(len(sample_chars))\n",
        "#     for i,c in enumerate(sample_chars):\n",
        "#       print(outputs[i])\n",
        "#       print(np.where(outputs[i] == max(outputs[i]), 1, 0))\n",
        "#       print(c)\n",
        "def segment(sample,net):\n",
        "    net.eval()\n",
        "    #sample ='ខ្ញុំទៅសាលារៀន'\n",
        "    sample_chars = list(sample)\n",
        "    sample_index = [chars2idx[x] for x in sample_chars ]\n",
        "    sample_arr = torch.from_numpy(np.array(sample_index)).unsqueeze(0)\n",
        "    sample_encoded = one_hot_encode(sample_arr,len(CHARS))\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        sample_encoded = sample_encoded.cuda()\n",
        "\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "    outputs,_ = net(sample_encoded,h)\n",
        "    outputs = torch.sigmoid(outputs)\n",
        "    print(len(outputs))\n",
        "    if(train_on_gpu):\n",
        "        outputs  =outputs.detach().cpu().numpy()\n",
        "    else:\n",
        "        outputs  =outputs.detach().numpy()\n",
        "    idx =(outputs>=0.5).astype(float)\n",
        "    print(outputs)\n",
        "    predicts = one_hot.inverse_transform(idx)\n",
        "    print(predicts)\n",
        "    result = ''\n",
        "    t = ''\n",
        "    for i, c in enumerate(sample_chars):\n",
        "        print(len(predicts[i]))\n",
        "        if (predicts[i][1] !=1):\n",
        "            # if result =='':\n",
        "            #     result +=c\n",
        "                # t = idx2tags[predicts[i][0]]\n",
        "            if(len(predicts[i])==2):\n",
        "                result +=t + ' ' +c\n",
        "                t= idx2tags[predicts[i][1]]\n",
        "            else:\n",
        "                result +='_'+ t + ' ' +c\n",
        "                t = idx2tags[predicts[i][1]]\n",
        "        else:\n",
        "            result +=c\n",
        "    return result+'_'+ t\n",
        "\n",
        "    # for i,c in enumerate(sample_chars):\n",
        "    #     if idx[i] != 1:\n",
        "    #         if result =='':\n",
        "    #             result +=c\n",
        "    #             t = idx2tags[idx[i]]\n",
        "    #         else:\n",
        "    #             result +='_'+ t + ' ' +c\n",
        "    #             t = idx2tags[idx[i]]\n",
        "    #     else:\n",
        "    #         result +=c\n",
        "    # return result+'_'+ t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-DpC-DEB0Cl"
      },
      "outputs": [],
      "source": [
        "#samples ='អាវថ្មី'\n",
        "samples = 'ការបាត់បង់'\n",
        "#samples = 'ចូរថែសុខភាពរបស់ខ្លួនឱ្យបានល្អត្រូវចេះអាណិតស្រឡាញ់ខ្លួនឱ្យបានច្រើនអាណិតស្រឡាញ់ដល់ក្រុមគ្រួសារគិតគូរដល់ទឹកចិត្តអ្នកដែលអាណិតស្រឡាញ់យើងនិងខ្លាចបាត់បង់យើងព្រោះបញ្ហាទាំងឡាយដែលបានកើតឡើងចំពោះជីវិតយើងហើយនោះទោះបីជាល្អក្តីអាក្រក់ក្តីវាមិនអាចសារត្រលប់ថយក្រោយដូចខ្សែអាត់សំឡេងឬខ្សែវីដេអូបាននោះទេ។'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9EN6RCqoFhI"
      },
      "outputs": [],
      "source": [
        "#print(np.argmax([[80, -0.36012337, -0.40979862, -0.4940329,  -0.41396353], [2, 3, 4], [10, 20, 30]], axis = 0))\n",
        "print(POSTAGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgA-E-358k90"
      },
      "outputs": [],
      "source": [
        "segment(samples, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq_xPAUtfzWI"
      },
      "outputs": [],
      "source": [
        "segment(samples, loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zfYAnotcJkx"
      },
      "outputs": [],
      "source": [
        "print(len(data[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWYPpbiGx6Ly"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYZaOo4kPLDo"
      },
      "outputs": [],
      "source": [
        "# def predict(test_dl, net):\n",
        "#     net.eval()\n",
        "#     test_result =[]\n",
        "#     for x, y in test_dl:\n",
        "#       x = one_hot_encode(x, len(CHARS))\n",
        "#       batch_size = x.shape[0]\n",
        "\n",
        "#       if(train_on_gpu):\n",
        "#           sample_encoded = x.cuda()\n",
        "\n",
        "#       h = net.init_hidden(batch_size)\n",
        "\n",
        "#       h = tuple([each.data for each in h])\n",
        "#       outputs,_ = net(sample_encoded,h)\n",
        "#       if(train_on_gpu):\n",
        "#           outputs  =outputs.detach().cpu().numpy()\n",
        "#       else:\n",
        "#           outputs  =outputs.detach().numpy()\n",
        "      \n",
        "#       idx = np.argmax(outputs, axis=1)\n",
        "#       test_result = [idx2tags[idx[i]] for i in idx]\n",
        "#       print(test_result)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pRsSDQZRiCh"
      },
      "outputs": [],
      "source": [
        "# predict(test_dl, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMPlj4GEAAaG"
      },
      "outputs": [],
      "source": [
        "# def accuracy(samples):\n",
        "#   pos_count = {pos: {\"correct\": 0, \"corpus\": 0} for pos in POSTAGS}\n",
        "#   for sample in samples:\n",
        "pos_count = {pos: {\"correct\": 0, \"corpus\": 0} for pos in POSTAGS}\n",
        "for num, sentence_tag in enumerate(tag_clean_all):\n",
        "  for i, tag in enumerate(sentence_tag):\n",
        "    tag_index = tags2idx[tag]\n",
        "    pos_count[idx2tags[tag_index]][\"corpus\"] += 1\n",
        "for num, sentence_tag in enumerate(tag_clean_all):\n",
        "  for i, tag in enumerate(sentence_tag):\n",
        "    tag_index = tags2idx[tag]\n",
        "    pos_count[idx2tags[tag_index]][\"correct\"] += 1\n",
        "print(pos_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7T553FnAGMP"
      },
      "outputs": [],
      "source": [
        "nototal_correct = 0\n",
        "total_corpus = 0\n",
        "for pos in pos_count:\n",
        "  if pos not in [\"ns\"]:\n",
        "    correct = pos_count[pos][\"correct\"]\n",
        "    corpus = pos_count[pos][\"corpus\"]\n",
        "    total_corpus += pos_count[pos][\"corpus\"]\n",
        "    total_correct += pos_count[pos][\"correct\"]\n",
        "    accuracy = round((correct / corpus)*100, 2)\n",
        "    print(f\"-- {pos}: {accuracy} | correct: {correct}, corpus: {corpus}\")\n",
        "\n",
        "accuracy = round((total_correct / total_corpus)*100, 2)\n",
        "print(f\"AVERAGE ACCURACY: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/Dataset/Khmer NLP/entire_model.pt'\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hcsrqijCrHt",
        "outputId": "3b044c3f-3c1a-48a2-b3da-4b0b6df086c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordSegPosTag(\n",
              "  (lstm): LSTM(168, 256, num_layers=5, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=25, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = 'ការបាត់បង់'"
      ],
      "metadata": {
        "id": "_E7a1WnmC0jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment(samples, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yoPZLyV1CtUW",
        "outputId": "82bdb864-b39c-4f32-a1c4-c8940d3b0084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "[[5.17615020e-01 1.07320095e-03 5.14833210e-03 2.28263810e-03\n",
            "  1.41419633e-03 2.67551537e-03 4.55194153e-04 7.26110069e-03\n",
            "  6.62135881e-06 2.60308874e-03 1.64227770e-03 3.66419728e-04\n",
            "  1.34887407e-03 1.15622068e-03 3.85895814e-03 2.56825029e-03\n",
            "  9.62938939e-04 1.91999215e-03 3.80407291e-05 7.55636647e-05\n",
            "  2.74640001e-07 6.83488033e-04 3.84194660e-04 7.88775142e-05\n",
            "  2.24063399e-07]\n",
            " [5.20923734e-01 8.64748359e-01 1.13723189e-01 3.28604616e-02\n",
            "  1.02545246e-02 2.91250106e-02 8.94946326e-03 8.68294039e-04\n",
            "  3.89446359e-06 7.63947656e-03 1.67932585e-02 3.82176670e-03\n",
            "  7.26047484e-03 9.14696872e-01 3.93798091e-02 9.99629870e-03\n",
            "  3.30062094e-03 8.98039434e-03 3.48631351e-04 1.28850748e-04\n",
            "  2.03856008e-08 1.25684298e-03 3.08754109e-03 9.48359157e-05\n",
            "  2.02374544e-08]\n",
            " [4.74360496e-01 9.94429231e-01 1.23835902e-03 4.74046741e-04\n",
            "  1.60236581e-04 4.75127978e-04 1.81058116e-04 6.27690542e-06\n",
            "  1.92040481e-08 3.96259275e-05 3.20690888e-04 7.86847377e-05\n",
            "  8.44337919e-05 9.94381845e-01 2.53641250e-04 1.06727042e-04\n",
            "  4.27909472e-05 7.52211563e-05 5.65867322e-06 1.67145708e-06\n",
            "  5.42357608e-12 8.22705988e-06 6.26689434e-05 1.07665801e-06\n",
            "  5.30641460e-12]\n",
            " [5.40052414e-01 2.89046586e-01 2.99871713e-01 1.22710563e-01\n",
            "  3.09686083e-02 9.17051584e-02 2.92988773e-02 1.29929138e-03\n",
            "  1.42231215e-06 7.28580356e-03 4.76945303e-02 1.04954187e-02\n",
            "  1.20655838e-02 5.78786373e-01 6.16561323e-02 2.04279814e-02\n",
            "  5.04539488e-03 1.06477775e-02 3.11107811e-04 7.97407338e-05\n",
            "  1.05327302e-08 4.98751469e-04 6.02281932e-03 6.79407531e-05\n",
            "  1.02225268e-08]\n",
            " [4.68077183e-01 9.98676002e-01 5.91614167e-04 2.58665765e-04\n",
            "  9.70283072e-05 2.78029649e-04 1.46608581e-04 2.86205022e-06\n",
            "  1.09937677e-08 1.50568931e-05 1.96268520e-04 5.85546259e-05\n",
            "  4.61896343e-05 9.98532295e-01 1.06409279e-04 4.97692308e-05\n",
            "  2.31594768e-05 3.65086089e-05 4.46653848e-06 1.04834055e-06\n",
            "  1.93850730e-12 3.64499488e-06 4.58434479e-05 6.29371527e-07\n",
            "  1.92983043e-12]\n",
            " [4.75349128e-01 9.97982502e-01 9.50103335e-04 3.69804446e-04\n",
            "  1.37186566e-04 4.12058173e-04 2.06790457e-04 3.31666160e-06\n",
            "  1.38859253e-08 2.24567593e-05 2.57448613e-04 8.04808296e-05\n",
            "  6.64546023e-05 9.97904897e-01 1.72384243e-04 7.40894538e-05\n",
            "  3.26453410e-05 5.68893010e-05 5.67650432e-06 1.40083955e-06\n",
            "  2.90560718e-12 5.07740424e-06 6.05647656e-05 7.08326240e-07\n",
            "  2.85326905e-12]\n",
            " [4.72940594e-01 9.98155296e-01 8.75367259e-04 3.44712840e-04\n",
            "  1.28306245e-04 3.83903243e-04 1.95174012e-04 3.14896010e-06\n",
            "  1.32924933e-08 2.07966877e-05 2.41275586e-04 7.59488612e-05\n",
            "  6.19500424e-05 9.98058617e-01 1.58336086e-04 6.87097781e-05\n",
            "  3.06100446e-05 5.25977739e-05 5.43914939e-06 1.33367780e-06\n",
            "  2.67127791e-12 4.76174091e-06 5.75627710e-05 6.78575702e-07\n",
            "  2.63988961e-12]\n",
            " [5.49235463e-01 5.92343882e-02 3.63578051e-01 2.27973402e-01\n",
            "  3.96335945e-02 1.22343577e-01 1.48702143e-02 1.27738034e-02\n",
            "  4.03888635e-06 2.53422987e-02 1.21382520e-01 1.15052089e-02\n",
            "  1.64361540e-02 2.34222218e-01 1.19050249e-01 4.56732623e-02\n",
            "  9.72861424e-03 1.49398623e-02 4.70261526e-04 1.57590053e-04\n",
            "  5.20011838e-08 9.92621644e-04 5.72991790e-03 2.86370720e-04\n",
            "  5.14211358e-08]\n",
            " [4.66032535e-01 9.98734534e-01 6.12112286e-04 2.90672819e-04\n",
            "  1.10439389e-04 3.11424170e-04 1.69480714e-04 3.50818368e-06\n",
            "  1.25872868e-08 1.56156602e-05 2.32180231e-04 6.89327208e-05\n",
            "  4.90914899e-05 9.98624444e-01 1.09542008e-04 5.33159873e-05\n",
            "  2.52887658e-05 3.78891054e-05 5.24461120e-06 1.16475326e-06\n",
            "  2.44304056e-12 3.58480406e-06 5.10025493e-05 7.74890509e-07\n",
            "  2.46891700e-12]\n",
            " [4.67637986e-01 9.98516738e-01 6.95714902e-04 2.81353510e-04\n",
            "  1.02520266e-04 3.04918969e-04 1.52892506e-04 2.64632831e-06\n",
            "  1.13784573e-08 1.73318913e-05 1.97434303e-04 6.06744215e-05\n",
            "  5.02050643e-05 9.98387575e-01 1.27838517e-04 5.60379231e-05\n",
            "  2.54547249e-05 4.27065606e-05 4.60304545e-06 1.13372414e-06\n",
            "  2.04999602e-12 4.06641175e-06 4.70886080e-05 5.85593511e-07\n",
            "  2.02660761e-12]]\n",
            "[(0,), (0, 1, 13), (1, 13), (0, 13), (1, 13), (1, 13), (1, 13), (0,), (1, 13), (1, 13)]\n",
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-79b3d4819c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-da2ecce3a721>\u001b[0m in \u001b[0;36msegment\u001b[0;34m(sample, net)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;31m# if result =='':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m#     result +=c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": " Final_Version_#5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}